
[+] General Parameters:
Training data:	data/msn1.fold1.train.5k.txt
Test data:	data/msn1.fold1.test.5k.txt
Validation data:	data/msn1.fold1.vali.5k.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	NDCG@10
Feature normalization: No

[+] LambdaMART's Parameters:
No. of trees: 100
No. of leaves: 8
No. of threshold candidates: -1
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [data/msn1.fold1.train.5k.txt]: 0... Reading feature file [data/msn1.fold1.train.5k.txt]... [Done.]            
(43 ranked lists, 5000 entries read)
Reading feature file [data/msn1.fold1.vali.5k.txt]: 0... Reading feature file [data/msn1.fold1.vali.5k.txt]... [Done.]            
(49 ranked lists, 5000 entries read)
Reading feature file [data/msn1.fold1.test.5k.txt]: 0... Reading feature file [data/msn1.fold1.test.5k.txt]... [Done.]            
(43 ranked lists, 5000 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.2197    | 0.1652    | 
2       | 0.2851    | 0.2415    | 
3       | 0.3577    | 0.264     | 
4       | 0.3755    | 0.2719    | 
5       | 0.4012    | 0.2886    | 
6       | 0.4091    | 0.2904    | 
7       | 0.4171    | 0.2947    | 
8       | 0.4281    | 0.3049    | 
9       | 0.437     | 0.3128    | 
10      | 0.4449    | 0.3175    | 
11      | 0.4548    | 0.3301    | 
12      | 0.4515    | 0.3351    | 
13      | 0.4562    | 0.3534    | 
14      | 0.4559    | 0.3537    | 
15      | 0.452     | 0.3645    | 
16      | 0.4533    | 0.367     | 
17      | 0.4511    | 0.3735    | 
18      | 0.4701    | 0.3777    | 
19      | 0.4745    | 0.3761    | 
20      | 0.472     | 0.3765    | 
21      | 0.4713    | 0.3782    | 
22      | 0.4711    | 0.3841    | 
23      | 0.4703    | 0.3853    | 
24      | 0.4656    | 0.3869    | 
25      | 0.4659    | 0.3936    | 
26      | 0.4664    | 0.3943    | 
27      | 0.4658    | 0.3932    | 
28      | 0.4702    | 0.3933    | 
29      | 0.4688    | 0.3901    | 
30      | 0.4663    | 0.3878    | 
31      | 0.4687    | 0.4119    | 
32      | 0.4726    | 0.418     | 
33      | 0.4728    | 0.4198    | 
34      | 0.4775    | 0.4179    | 
35      | 0.4824    | 0.4141    | 
36      | 0.4835    | 0.4145    | 
37      | 0.4876    | 0.4161    | 
38      | 0.4897    | 0.4139    | 
39      | 0.499     | 0.4151    | 
40      | 0.5005    | 0.4198    | 
41      | 0.5051    | 0.4087    | 
42      | 0.505     | 0.4105    | 
43      | 0.511     | 0.4117    | 
44      | 0.5149    | 0.4064    | 
45      | 0.5157    | 0.4064    | 
46      | 0.5234    | 0.4093    | 
47      | 0.5259    | 0.4114    | 
48      | 0.5296    | 0.4116    | 
49      | 0.5345    | 0.4137    | 
50      | 0.5403    | 0.4157    | 
51      | 0.5406    | 0.4181    | 
52      | 0.5468    | 0.415     | 
53      | 0.5448    | 0.4122    | 
54      | 0.5494    | 0.4158    | 
55      | 0.5521    | 0.4148    | 
56      | 0.5518    | 0.4174    | 
57      | 0.5502    | 0.4184    | 
58      | 0.5503    | 0.4267    | 
59      | 0.552     | 0.4253    | 
60      | 0.5518    | 0.4211    | 
61      | 0.5526    | 0.4173    | 
62      | 0.5513    | 0.4163    | 
63      | 0.5533    | 0.4141    | 
64      | 0.5528    | 0.4134    | 
65      | 0.5496    | 0.4207    | 
66      | 0.5563    | 0.4213    | 
67      | 0.558     | 0.422     | 
68      | 0.5607    | 0.4216    | 
69      | 0.5627    | 0.4217    | 
70      | 0.5674    | 0.4183    | 
71      | 0.5661    | 0.4215    | 
72      | 0.5657    | 0.426     | 
73      | 0.5764    | 0.4293    | 
74      | 0.5784    | 0.4251    | 
75      | 0.5788    | 0.4272    | 
76      | 0.5824    | 0.4255    | 
77      | 0.5816    | 0.4253    | 
78      | 0.5863    | 0.4268    | 
79      | 0.5887    | 0.4212    | 
80      | 0.593     | 0.4197    | 
81      | 0.5938    | 0.42      | 
82      | 0.6012    | 0.4283    | 
83      | 0.6051    | 0.4294    | 
84      | 0.605     | 0.4291    | 
85      | 0.6041    | 0.4303    | 
86      | 0.609     | 0.4253    | 
87      | 0.6091    | 0.426     | 
88      | 0.6091    | 0.4289    | 
89      | 0.6103    | 0.4292    | 
90      | 0.61      | 0.43      | 
91      | 0.6124    | 0.4313    | 
92      | 0.6109    | 0.4335    | 
93      | 0.6105    | 0.433     | 
94      | 0.6131    | 0.4308    | 
95      | 0.6164    | 0.4296    | 
96      | 0.6149    | 0.4319    | 
97      | 0.6164    | 0.4341    | 
98      | 0.6191    | 0.4299    | 
99      | 0.6176    | 0.4313    | 
100     | 0.6199    | 0.4318    | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.6164
NDCG@10 on validation data: 0.4341
---------------------------------
NDCG@10 on test data: 0.3481
